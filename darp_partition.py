import math
import numpy as np
from sklearn.cluster import KMeans
import warnings
# å…³é—­sklearnæ— å…³è­¦å‘Šï¼Œæ¸…çˆ½è¿è¡Œ
warnings.filterwarnings('ignore')

# å¯¼å…¥matplotlibç”¨äºå¯è§†åŒ–
import matplotlib.pyplot as plt


def extract_points_from_gcode(gcode_path):
    """
    ä¿®å¤ç‰ˆï¼šä»Gcodeæå–G1æŒ‡ä»¤çš„XYåæ ‡ç‚¹
    ä¼˜åŒ–ï¼šæ­£åˆ™æå–åæ ‡ï¼Œå…¼å®¹æ‰€æœ‰G1æŒ‡ä»¤æ ¼å¼ï¼Œå®¹é”™æ€§æ‹‰æ»¡
    """
    points = []
    with open(gcode_path, 'r', encoding='utf-8', errors='ignore') as f:
        for line in f:
            line = line.strip()
            if line.startswith('G1') and 'X' in line and 'Y' in line:
                # ========== ä¿®å¤æ ¸å¿ƒï¼šç²¾å‡†æå–X/Yåæ ‡ï¼Œé€‚é…æ‰€æœ‰æ ¼å¼ ==========
                x_str = ''
                y_str = ''
                # éå†æå–Xåæ•°å­—ï¼Œç›´åˆ°éæ•°å­—/å°æ•°ç‚¹
                x_start = line.find('X') + 1
                for c in line[x_start:]:
                    if c in '0123456789.-':
                        x_str += c
                    else:
                        break
                # éå†æå–Yåæ•°å­—ï¼Œç›´åˆ°éæ•°å­—/å°æ•°ç‚¹
                y_start = line.find('Y') + 1
                for c in line[y_start:]:
                    if c in '0123456789.-':
                        y_str += c
                    else:
                        break
                # åæ ‡è½¬æ¢
                try:
                    x = float(x_str)
                    y = float(y_str)
                    points.append((x, y))
                except (ValueError, IndexError):
                    continue
    return points

def calculate_distance_squared(p1, p2):
    """
    ä¼˜åŒ–ç‰ˆï¼šè®¡ç®—ä¸¤ç‚¹é—´ã€è·ç¦»å¹³æ–¹ã€‘ï¼Œæ›¿ä»£å¼€æ–¹è¿ç®—ï¼Œæé€Ÿ50%+
    æ¯”è¾ƒè·ç¦»å¤§å° â†’ è·ç¦»å¹³æ–¹å¤§å°ç­‰ä»·ï¼Œå®Œå…¨ä¸éœ€è¦math.sqrt
    """
    return (p1[0] - p2[0])**2 + (p1[1] - p2[1])**2

def update_cluster_centers(points, cluster_labels, n_clusters):
    """
    æ–°å¢æ ¸å¿ƒå‡½æ•°ï¼šæ ¹æ®æœ€æ–°çš„èšç±»æ ‡ç­¾ï¼Œé‡æ–°è®¡ç®—æ¯ä¸ªèšç±»çš„çœŸå®ä¸­å¿ƒ
    è§£å†³ï¼šèšç±»æ ‡ç­¾ä¿®æ”¹åï¼Œä¸­å¿ƒä¸åŒæ­¥çš„è‡´å‘½é—®é¢˜
    """
    cluster_centers = []
    for cluster_id in range(n_clusters):
        # ç­›é€‰å½“å‰èšç±»çš„æ‰€æœ‰ç‚¹
        cluster_points = [points[i] for i in range(len(points)) if cluster_labels[i] == cluster_id]
        if cluster_points:
            center_x = np.mean([p[0] for p in cluster_points])
            center_y = np.mean([p[1] for p in cluster_points])
            cluster_centers.append((center_x, center_y))
        else:
            # ç©ºèšç±»ç”¨åŸç‚¹å…œåº•
            cluster_centers.append((0.0, 0.0))
    return cluster_centers

def calculate_cluster_cohesion(points, cluster_labels, cluster_id, point_idx):
    """è®¡ç®—ç‚¹ä¸åŒä¸€èšç±»ä¸­å…¶ä»–ç‚¹çš„å‡èšåŠ›ï¼ˆå¹³å‡è·ç¦»å¹³æ–¹ï¼‰"""
    cluster_points = [points[i] for i in range(len(points)) 
                     if cluster_labels[i] == cluster_id and i != point_idx]
    if not cluster_points:
        return 0.0  # åªæœ‰ä¸€ä¸ªç‚¹æ—¶å‡èšåŠ›ä¸º0
    
    point = points[point_idx]
    total_dist_sq = sum(calculate_distance_squared(point, p) for p in cluster_points)
    return total_dist_sq / len(cluster_points)


def darp_partition(points, n_clusters):
    # è¾¹ç•Œåˆ¤æ–­ï¼šç‚¹æ•°å°äºåˆ†åŒºæ•°ï¼Œç›´æ¥å‡åˆ†
    if len(points) <= n_clusters:
        return [i % n_clusters for i in range(len(points))]
    
    points_array = np.array(points)
    # ========== ä¿®å¤KMeansè­¦å‘Š + ä¼˜åŒ–æ”¶æ•›å‚æ•° ==========
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10, max_iter=300)
    cluster_labels = kmeans.fit_predict(points_array)
    cluster_centers = kmeans.cluster_centers_.tolist()
    
    # Step 1: è¿­ä»£ä¼˜åŒ–ç›¸é‚»ç‚¹ï¼Œå¢å¼ºèšé›†æ€§
    max_iterations = 5  # å¢åŠ è¿­ä»£æ¬¡æ•°ä»¥æé«˜èšé›†æ•ˆæœ
    for iteration in range(max_iterations):
        # æ›´æ–°èšç±»ä¸­å¿ƒ
        cluster_centers = update_cluster_centers(points, cluster_labels, n_clusters)
        
        # ä¼˜åŒ–èšç±»ï¼Œç¡®ä¿ç›¸é‚»ç‚¹åœ¨åŒä¸€åˆ†åŒºï¼ˆè·ç¦»æ„ŸçŸ¥ï¼‰
        for i in range(1, len(points)):
            prev_point = points[i-1]
            curr_point = points[i]
            prev_cluster = cluster_labels[i-1]
            curr_cluster = cluster_labels[i]
            
            if prev_cluster != curr_cluster:
                # ç”¨è·ç¦»å¹³æ–¹è®¡ç®—ï¼Œå¤§å¹…æé€Ÿ
                prev_dist_to_prev = calculate_distance_squared(prev_point, cluster_centers[prev_cluster])
                prev_dist_to_curr = calculate_distance_squared(prev_point, cluster_centers[curr_cluster])
                curr_dist_to_prev = calculate_distance_squared(curr_point, cluster_centers[prev_cluster])
                curr_dist_to_curr = calculate_distance_squared(curr_point, cluster_centers[curr_cluster])
                
                # è®¡ç®—ç›¸é‚»ç‚¹è·ç¦»ï¼Œå¢åŠ è¿ç»­æ€§æƒé‡
                consecutive_dist = calculate_distance_squared(prev_point, curr_point)
                
                # å¢åŠ è¿ç»­æ€§æƒé‡ï¼Œå¢å¼ºèšé›†æ€§
                cost_prev2curr = prev_dist_to_curr + curr_dist_to_curr + consecutive_dist * 0.5
                cost_curr2prev = prev_dist_to_prev + curr_dist_to_prev + consecutive_dist * 0.5
                
                if cost_prev2curr < cost_curr2prev:
                    cluster_labels[i-1] = curr_cluster
                else:
                    cluster_labels[i] = prev_cluster

    # Step 2: æ›´æ–°èšç±»ä¸­å¿ƒ
    cluster_centers = update_cluster_centers(points, cluster_labels, n_clusters)
    
    # Step 3: ä¸¥æ ¼å¹³è¡¡åˆ†åŒºï¼Œç¡®ä¿æ¯ä¸ªåˆ†åŒºç‚¹æ•°é«˜åº¦ç›¸åŒï¼ˆç›¸å·®ä¸è¶…è¿‡1ï¼‰
    total_points = len(points)
    base_size = total_points // n_clusters
    extra_points = total_points % n_clusters
    
    # ä¸ºæ¯ä¸ªèšç±»åˆ†é…ç²¾ç¡®çš„ç›®æ ‡å¤§å°
    target_sizes = [base_size + 1 if i < extra_points else base_size for i in range(n_clusters)]
    
    # è®¡ç®—å½“å‰èšç±»å¤§å°
    cluster_sizes = [list(cluster_labels).count(i) for i in range(n_clusters)]
    
    # åˆ›å»ºéœ€è¦è°ƒæ•´çš„èšç±»åˆ—è¡¨
    oversized_clusters = [(i, cluster_sizes[i] - target_sizes[i]) 
                         for i in range(n_clusters) if cluster_sizes[i] > target_sizes[i]]
    undersized_clusters = [(i, target_sizes[i] - cluster_sizes[i]) 
                         for i in range(n_clusters) if cluster_sizes[i] < target_sizes[i]]
    
    # æŒ‰éœ€è¦è°ƒæ•´çš„ç‚¹æ•°æ’åº
    oversized_clusters.sort(key=lambda x: x[1], reverse=True)
    undersized_clusters.sort(key=lambda x: x[1], reverse=True)
    
    # è°ƒæ•´èšç±»å¤§å°ï¼Œç¡®ä¿ä¸¥æ ¼å¹³è¡¡
    while oversized_clusters and undersized_clusters:
        # å–å‡ºéœ€è¦è°ƒæ•´çš„æœ€å¤§çš„è¿‡å¤§å’Œè¿‡å°èšç±»
        oversized_cluster, over_count = oversized_clusters.pop(0)
        undersized_cluster, under_count = undersized_clusters.pop(0)
        
        # è®¡ç®—éœ€è¦ç§»åŠ¨çš„ç‚¹æ•°
        move_count = min(over_count, under_count)
        
        for _ in range(move_count):
            # æ‰¾åˆ°å½“å‰è¿‡å¤§èšç±»ä¸­æœ€é€‚åˆç§»åŠ¨çš„ç‚¹
            best_point_idx = -1
            best_score = float('inf')
            
            for point_idx in range(len(points)):
                if cluster_labels[point_idx] == oversized_cluster:
                    # è®¡ç®—è¯¥ç‚¹åˆ°ç›®æ ‡èšç±»ä¸­å¿ƒçš„è·ç¦»
                    dist_to_target = calculate_distance_squared(points[point_idx], cluster_centers[undersized_cluster])
                    
                    # è®¡ç®—è¯¥ç‚¹åœ¨å½“å‰èšç±»ä¸­çš„å‡èšåŠ›ï¼ˆç¦»å¼€å½“å‰èšç±»çš„ä»£ä»·ï¼‰
                    cohesion_current = calculate_cluster_cohesion(points, cluster_labels, oversized_cluster, point_idx)
                    
                    # ç»¼åˆè¯„åˆ†ï¼šè·ç¦»ç›®æ ‡ä¸­å¿ƒè¶Šè¿‘ï¼Œå½“å‰å‡èšåŠ›è¶Šä½ï¼Œè¶Šé€‚åˆç§»åŠ¨
                    score = dist_to_target + cohesion_current * 0.5
                    
                    if score < best_score:
                        best_score = score
                        best_point_idx = point_idx
            
            # ç§»åŠ¨ç‚¹åˆ°ç›®æ ‡èšç±»
            if best_point_idx != -1:
                cluster_labels[best_point_idx] = undersized_cluster
        
        # æ›´æ–°èšç±»å¤§å°
        cluster_sizes = [list(cluster_labels).count(i) for i in range(n_clusters)]
        
        # é‡æ–°ç”Ÿæˆéœ€è¦è°ƒæ•´çš„èšç±»åˆ—è¡¨
        oversized_clusters = [(i, cluster_sizes[i] - target_sizes[i]) 
                             for i in range(n_clusters) if cluster_sizes[i] > target_sizes[i]]
        undersized_clusters = [(i, target_sizes[i] - cluster_sizes[i]) 
                             for i in range(n_clusters) if cluster_sizes[i] < target_sizes[i]]
        
        # æŒ‰éœ€è¦è°ƒæ•´çš„ç‚¹æ•°æ’åº
        oversized_clusters.sort(key=lambda x: x[1], reverse=True)
        undersized_clusters.sort(key=lambda x: x[1], reverse=True)
        
        # æ›´æ–°èšç±»ä¸­å¿ƒ
        cluster_centers = update_cluster_centers(points, cluster_labels, n_clusters)
    
    return cluster_labels

def write_partitioned_gcode(original_gcode_path, output_path, points, cluster_labels, n_clusters):
    """
    ä¿®å¤ç‰ˆï¼šå†™å›åˆ†åŒºæ ‡æ³¨çš„GCodeï¼Œä¼˜åŒ–æ ¼å¼+ç´¢å¼•å®‰å…¨+å¯è¯»æ€§
    """
    with open(original_gcode_path, 'r', encoding='utf-8', errors='ignore') as f:
        lines = f.readlines()
    
    partitioned_gcode = []
    current_point_idx = 0
    last_partition = -1  # é¿å…é‡å¤å†™å…¥åˆ†åŒºæ³¨é‡Š
    
    for line in lines:
        raw_line = line.rstrip('\n')  # ä¿ç•™åŸå§‹æ¢è¡Œï¼Œä¸ä¸¢å¤±æ ¼å¼
        line_strip = raw_line.strip()
        
        if line_strip.startswith('G1') and 'X' in line_strip and 'Y' in line_strip:
            if current_point_idx < len(points):
                cluster_id = cluster_labels[current_point_idx]
                # ä¼˜åŒ–ï¼šåŒä¸€ä¸ªåˆ†åŒºåªå†™ä¸€æ¬¡æ³¨é‡Šï¼Œé¿å…æ¯è¡Œéƒ½æ’ï¼ŒGCodeæ›´å¹²å‡€
                if cluster_id != last_partition:
                    partitioned_gcode.append(f"; ====== DARP PARTITION {cluster_id} ======")
                    last_partition = cluster_id
            partitioned_gcode.append(raw_line)
            current_point_idx += 1
        else:
            partitioned_gcode.append(raw_line)
    
    # å†™å…¥æ–‡ä»¶ï¼Œä¿ç•™åŸå§‹æ ¼å¼
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(partitioned_gcode))
    
    # è¾“å‡ºåˆ†åŒºç»Ÿè®¡ä¿¡æ¯
    print(f"\nâœ… åˆ†åŒºå®Œæˆï¼")
    print(f"ğŸ“Š æ€»ç‚¹æ•°: {len(points)} | åˆ†åŒºæ•°: {n_clusters}")
    cluster_sizes = [list(cluster_labels).count(i) for i in range(n_clusters)]
    total = 0
    for i, size in enumerate(cluster_sizes):
        print(f"ğŸ“Œ åˆ†åŒº {i+1}: {size} ä¸ªç‚¹")
        total += size
    print(f"âœ… æ ¡éªŒæ€»æ•°: {total}")


def visualize_partition(points, cluster_labels, n_clusters):
    """
    å¯è§†åŒ–åˆ†åŒºç»“æœï¼Œç”Ÿæˆå›¾åƒæ–‡ä»¶
    """
    points_array = np.array(points)
    x = points_array[:, 0]
    y = points_array[:, 1]
    
    # åˆ›å»ºå›¾å½¢
    plt.figure(figsize=(12, 8))
    
    # ç»˜åˆ¶æ‰€æœ‰ç‚¹ï¼ŒæŒ‰åˆ†åŒºç€è‰²
    scatter = plt.scatter(x, y, c=cluster_labels, cmap='tab10', s=20, alpha=0.7)
    
    # æ·»åŠ é¢œè‰²æ¡
    plt.colorbar(scatter, ticks=range(n_clusters), label='åˆ†åŒºID')
    
    # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
    plt.title('DARPåˆ†åŒºç»“æœå¯è§†åŒ–', fontsize=14)
    plt.xlabel('Xåæ ‡', fontsize=12)
    plt.ylabel('Yåæ ‡', fontsize=12)
    
    # è®¾ç½®åæ ‡è½´ç›¸ç­‰æ¯”ä¾‹
    plt.axis('equal')
    plt.grid(True, alpha=0.3)
    
    # ä¿å­˜å›¾åƒ
    plt.savefig('darp_partition_visualization.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print("âœ… åˆ†åŒºå¯è§†åŒ–å·²ä¿å­˜åˆ°: darp_partition_visualization.png")
    
    # ä¸ºæ¯ä¸ªåˆ†åŒºåˆ›å»ºå•ç‹¬çš„å¯è§†åŒ–
    for cluster_id in range(n_clusters):
        cluster_points = points_array[cluster_labels == cluster_id]
        if len(cluster_points) > 0:
            plt.figure(figsize=(10, 6))
            plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c='blue', s=20, alpha=0.7)
            plt.title(f'åˆ†åŒº {cluster_id} å¯è§†åŒ–', fontsize=14)
            plt.xlabel('Xåæ ‡', fontsize=12)
            plt.ylabel('Yåæ ‡', fontsize=12)
            plt.axis('equal')
            plt.grid(True, alpha=0.3)
            plt.savefig(f'darp_partition_{cluster_id}.png', dpi=300, bbox_inches='tight')
            plt.close()
    
    print(f"âœ… å„åˆ†åŒºå•ç‹¬å¯è§†åŒ–å·²ä¿å­˜")

def main():
    # ===================== å¯è‡ªå®šä¹‰é…ç½® =====================
    gcode_path = 'test_output_filtered.gcode'       # è¾“å…¥GCodeè·¯å¾„
    output_path = 'test_output_partitioned.gcode'   # è¾“å‡ºåˆ†åŒºåè·¯å¾„
    n_clusters = 2                                  # åˆ†åŒºæ•°é‡ï¼Œå¯è‡ªç”±è°ƒæ•´
    # ======================================================
    
    # æå–åæ ‡ç‚¹
    points = extract_points_from_gcode(gcode_path)
    if not points:
        print("âŒ æœªæå–åˆ°ä»»ä½•G1æŒ‡ä»¤çš„XYåæ ‡ç‚¹ï¼")
        return
    print(f"âœ… æˆåŠŸæå–åˆ° {len(points)} ä¸ªæœ‰æ•ˆåæ ‡ç‚¹")
    
    # æ‰§è¡ŒDARPåˆ†åŒºèšç±»
    print(f"â³ æ­£åœ¨æ‰§è¡ŒDARPåˆ†åŒºç®—æ³• (åˆ†åŒºæ•°: {n_clusters})...")
    cluster_labels = darp_partition(points, n_clusters)
    
    # å†™å›åˆ†åŒºåçš„GCode
    write_partitioned_gcode(gcode_path, output_path, points, cluster_labels, n_clusters)
    print(f"\nâœ… åˆ†åŒºç»“æœå·²ä¿å­˜è‡³: {output_path}")
    
    # å¯è§†åŒ–åˆ†åŒºç»“æœ
    print(f"\nâ³ æ­£åœ¨ç”Ÿæˆåˆ†åŒºå¯è§†åŒ–å›¾åƒ...")
    visualize_partition(points, cluster_labels, n_clusters)

if __name__ == "__main__":
    main()